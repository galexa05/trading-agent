{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fea793a",
   "metadata": {},
   "source": [
    "# Financial News Summarization Agent - Step by Step\n",
    "\n",
    "This notebook walks through the process of using the Financial News Summarization Agent to retrieve, summarize, and evaluate financial news articles related to a stock portfolio.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Setup and imports\n",
    "2. Initialize the vector database connection\n",
    "3. Initialize summarization models (baseline and fine-tuned)\n",
    "4. Retrieve articles for a portfolio\n",
    "5. Generate summaries using both models\n",
    "6. Evaluate the quality of summaries\n",
    "7. Visualize and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ba280",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62748bc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Add the parent directory to path to import our scripts\u001b[39;00m\n\u001b[32m     17\u001b[39m sys.path.append(os.path.join(os.path.dirname(\u001b[33m'\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvector_db_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorDatabaseManager\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI, HuggingFaceHub\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Projects/Trading-Agent/trading-agent/jupyter_files/../scripts/vector_db_manager.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple, Optional, Any, Union\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Add the parent directory to the path so we can import from trading-agent\u001b[39;00m\n\u001b[32m     21\u001b[39m sys.path.append(os.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/sentence_transformers/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m2.7.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m __MODEL_HUB_ORGANIZATION__ = \u001b[33m\"\u001b[39m\u001b[33msentence-transformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceTransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/sentence_transformers/datasets/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mInputExample\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/__init__.py:2611\u001b[39m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2613\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_meta_registrations.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     _add_op_to_registry,\n\u001b[32m     14\u001b[39m     _convert_out_params,\n\u001b[32m     15\u001b[39m     global_decomposition_table,\n\u001b[32m     16\u001b[39m     meta_table,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_decomp/__init__.py:276\u001b[39m\n\u001b[32m    272\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_aten_decompositions\u001b[39m() -> \u001b[33m\"\u001b[39m\u001b[33mCustomDecompTable\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_decomp/decompositions.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_prims/__init__.py:723\u001b[39m\n\u001b[32m    707\u001b[39m clone = _make_prim(\n\u001b[32m    708\u001b[39m     schema=\u001b[33m\"\u001b[39m\u001b[33mclone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    709\u001b[39m     meta=_clone_meta,\n\u001b[32m   (...)\u001b[39m\u001b[32m    713\u001b[39m     register_conj_neg_fallthrough=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    714\u001b[39m )\n\u001b[32m    716\u001b[39m digamma = _make_elementwise_unary_prim(\n\u001b[32m    717\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdigamma\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    718\u001b[39m     impl_aten=torch.digamma,\n\u001b[32m    719\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    720\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    721\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m erf = \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43merf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m erf_inv = _make_elementwise_unary_prim(\n\u001b[32m    731\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33merf_inv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    732\u001b[39m     impl_aten=torch.special.erfinv,\n\u001b[32m    733\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    734\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    735\u001b[39m )\n\u001b[32m    737\u001b[39m erfc = _make_elementwise_unary_prim(\n\u001b[32m    738\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33merfc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    739\u001b[39m     impl_aten=torch.special.erfc,\n\u001b[32m    740\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    741\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    742\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_prims/__init__.py:493\u001b[39m, in \u001b[36m_make_elementwise_unary_prim\u001b[39m\u001b[34m(name, type_promotion, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_elementwise_unary_prim\u001b[39m(\n\u001b[32m    487\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m, *, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, **kwargs\n\u001b[32m    488\u001b[39m ):\n\u001b[32m    489\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_prims/__init__.py:321\u001b[39m, in \u001b[36m_make_prim\u001b[39m\u001b[34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m     mutates_args = [\n\u001b[32m    317\u001b[39m         arg.name\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema.arguments\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m arg.alias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg.alias_info.is_write\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     prim_def = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims::\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     prim_def.register_fake(meta)\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_library/custom_ops.py:173\u001b[39m, in \u001b[36mcustom_op\u001b[39m\u001b[34m(name, fn, mutates_args, device_types, schema)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_library/custom_ops.py:154\u001b[39m, in \u001b[36mcustom_op.<locals>.inner\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m    151\u001b[39m     schema_str = schema\n\u001b[32m    153\u001b[39m namespace, opname = name.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m result = \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[32m    157\u001b[39m     expected = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_library/custom_ops.py:204\u001b[39m, in \u001b[36mCustomOpDef.__init__\u001b[39m\u001b[34m(self, namespace, name, schema, fn)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m._autocast_cpu_dtype: Optional[_dtype] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._lib = get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m._namespace, \u001b[38;5;28mself\u001b[39m._name)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_register_to_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._disabled_kernel: \u001b[38;5;28mset\u001b[39m = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    206\u001b[39m OPDEFS[\u001b[38;5;28mself\u001b[39m._qualname] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/_library/custom_ops.py:606\u001b[39m, in \u001b[36mCustomOpDef._register_to_dispatcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.has_kwarg_only_tensors(cpp_schema):\n\u001b[32m    598\u001b[39m     \u001b[38;5;66;03m# If you want to support this, the progression is:\u001b[39;00m\n\u001b[32m    599\u001b[39m     \u001b[38;5;66;03m# - supporting kwarg-only Tensors that are non-differentiable\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# - supporting kwarg-only Tensors (regardless of differentiability)\u001b[39;00m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    602\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcustom_op with kwarg-only Tensor args. Please make your \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    603\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtensors not kwarg-only. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    604\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpt2_compliant_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mneeds_fixed_stride_order\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[38;5;28mself\u001b[39m._opoverload = utils.lookup_op(\u001b[38;5;28mself\u001b[39m._qualname)\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfake_impl\u001b[39m(*args, **kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/trading-agent-Iwjhydeb/lib/python3.12/site-packages/torch/library.py:172\u001b[39m, in \u001b[36mLibrary.define\u001b[39m\u001b[34m(self, schema, alias_analysis, tags)\u001b[39m\n\u001b[32m    167\u001b[39m packet_name = name.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m name\n\u001b[32m    168\u001b[39m has_preexisting_packet = \u001b[38;5;28mhasattr\u001b[39m(torch.ops, \u001b[38;5;28mself\u001b[39m.ns) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(torch.ops, \u001b[38;5;28mself\u001b[39m.ns), packet_name\n\u001b[32m    170\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_analysis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m name = schema.split(\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    174\u001b[39m qualname = \u001b[38;5;28mself\u001b[39m.ns + \u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m + name\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ROUGE metrics for evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Add the parent directory to path to import our scripts\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), \"..\"))\n",
    "from scripts.vector_db_manager import VectorDatabaseManager\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms import OpenAI, HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure paths\n",
    "DB_PATH = \"../data/chroma_db\"\n",
    "COLLECTION_NAME = \"financial_articles\"\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"../data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a154e3",
   "metadata": {},
   "source": [
    "## 2. Initialize Vector Database\n",
    "\n",
    "First, we'll connect to our vector database to retrieve relevant financial news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318705b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector database connection\n",
    "vector_db = VectorDatabaseManager(\n",
    "    db_path=DB_PATH,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# Check how many documents are in the collection\n",
    "collection_count = vector_db.collection.count()\n",
    "print(f\"Collection '{COLLECTION_NAME}' contains {collection_count} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91307da",
   "metadata": {},
   "source": [
    "## 3. Initialize Summarization Models\n",
    "\n",
    "Now, we'll set up both baseline and fine-tuned summarization models. We'll use LangChain to create a flexible pipeline that can work with different model backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a58931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ROUGE scorer for evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize baseline model using HuggingFace Hub\n",
    "# Note: If you have an OpenAI API key, you can use that instead\n",
    "try:\n",
    "    # Check if environment variable exists\n",
    "    huggingface_api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if openai_api_key:\n",
    "        # Use OpenAI as baseline if API key is available\n",
    "        baseline_llm = OpenAI(temperature=0.3, openai_api_key=openai_api_key)\n",
    "        print(\"Using OpenAI as baseline model\")\n",
    "    else:\n",
    "        # Use an open-source model from HuggingFace as fallback\n",
    "        baseline_llm = HuggingFaceHub(\n",
    "            repo_id=\"facebook/bart-large-cnn\", \n",
    "            huggingfacehub_api_token=huggingface_api_token,\n",
    "            model_kwargs={\"temperature\": 0.3, \"max_length\": 500}\n",
    "        )\n",
    "        print(\"Using HuggingFace BART model as baseline\")\n",
    "    \n",
    "    # Create baseline prompt template\n",
    "    baseline_prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"Summarize the following financial news article in a concise, factual manner:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create baseline summarization chain\n",
    "    # baseline_chain = LLMChain(\n",
    "    #     llm=baseline_llm,\n",
    "    #     prompt=baseline_prompt\n",
    "    # )\n",
    "    baseline_chain = baseline_prompt | baseline_llm\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing model: {str(e)}\")\n",
    "    print(\"\\nAlternative: You can install a local model like 't5-small' to proceed without API keys.\")\n",
    "    baseline_chain = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    token=huggingface_api_token,\n",
    ")\n",
    "\n",
    "result = client.summarization(articles[0]['metadata']['summary'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# result = client.text_generation(\n",
    "#     model=\"facebook/bart-large-cnn\",\n",
    "#     task = 'summarization',\n",
    "#     prompt=\"Summarize: Your article text here.\",\n",
    "#     max_new_tokens=100\n",
    "# )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuned model, we would normally load a model from disk\n",
    "# For this demo, we'll use a different pre-trained model as a stand-in\n",
    "\n",
    "try:\n",
    "    # Check if we have a locally fine-tuned model\n",
    "    finetuned_model_path = os.path.join(MODEL_DIR, \"finetuned_summarizer\")\n",
    "    \n",
    "    if os.path.exists(finetuned_model_path):\n",
    "        # Load the local fine-tuned model\n",
    "        finetuned_llm = HuggingFaceHub(\n",
    "            repo_id=finetuned_model_path,\n",
    "            huggingfacehub_api_token=huggingface_api_token,\n",
    "            model_kwargs={\"temperature\": 0.3, \"max_length\": 150}\n",
    "        )\n",
    "        print(f\"Using fine-tuned model from {finetuned_model_path}\")\n",
    "    else:\n",
    "        # Use a different pre-trained model as our \"fine-tuned\" model for demonstration\n",
    "        finetuned_llm = HuggingFaceHub(\n",
    "            repo_id=\"sshleifer/distilbart-cnn-12-6\",  # smaller, \"fine-tuned\" version\n",
    "            huggingfacehub_api_token=huggingface_api_token,\n",
    "            model_kwargs={\"temperature\": 0.3, \"max_length\": 150}\n",
    "        )\n",
    "        print(\"Using DistilBART CNN as simulated fine-tuned model\")\n",
    "    \n",
    "    # Create finetuned prompt template (similar to baseline for this demo)\n",
    "    finetuned_prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template= \\\n",
    "    \"\"\"Summarize this financial news article focusing on key market insights, company performance, \n",
    "    and investor implications:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Summary:\"\"\"\n",
    "        )\n",
    "    \n",
    "    # Create finetuned summarization chain\n",
    "    finetuned_chain = LLMChain(\n",
    "        llm=finetuned_llm,\n",
    "        prompt=finetuned_prompt\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing fine-tuned model: {str(e)}\")\n",
    "    finetuned_chain = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96213c",
   "metadata": {},
   "source": [
    "## 4. Retrieve Articles for Portfolio\n",
    "\n",
    "Now, let's define our portfolio of stocks and retrieve relevant articles from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for_portfolio(tickers, days_back=7, article_limit=5):\n",
    "    \"\"\"\n",
    "    Retrieve articles related to the stock portfolio from the vector database.\n",
    "    \n",
    "    Args:\n",
    "        tickers: List of stock tickers to search for\n",
    "        days_back: How many days back to search\n",
    "        article_limit: Max number of articles per ticker\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing article info\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    cutoff_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Create a query to find articles about this ticker\n",
    "        query = f\"{ticker} financial news stock market\"\n",
    "        print(f\"Searching for articles about {ticker}...\")\n",
    "        \n",
    "        # Search the vector database\n",
    "        results = vector_db.query_database(\n",
    "            query=query,\n",
    "            n_results=article_limit,\n",
    "            metadata_filter=None,  # Can filter by date here if needed\n",
    "            include_summary=True\n",
    "        )\n",
    "        \n",
    "        # Process and filter results\n",
    "        documents = results.get(\"documents\", [])\n",
    "        metadatas = results.get(\"metadatas\", [])\n",
    "        distances = results.get(\"distances\", [])\n",
    "        \n",
    "        # Handle nested result structure from ChromaDB\n",
    "        if documents and isinstance(documents, list) and documents and isinstance(documents[0], list):\n",
    "            documents = documents[0]  # Take first query results\n",
    "            \n",
    "        if metadatas and isinstance(metadatas, list) and metadatas and isinstance(metadatas[0], list):\n",
    "            metadatas = metadatas[0]  # Take first query results\n",
    "            \n",
    "        if distances and isinstance(distances, list) and distances and isinstance(distances[0], list):\n",
    "            distances = distances[0]  # Take first query results\n",
    "            \n",
    "        # Add articles to results\n",
    "        if documents:\n",
    "            for i, doc in enumerate(documents):\n",
    "                metadata = metadatas[i]\n",
    "                article_info = {\n",
    "                    \"ticker\": ticker,\n",
    "                    \"document\": doc,\n",
    "                    \"metadata\": metadata,\n",
    "                    \"similarity\": 1.0 - distances[i]\n",
    "                }\n",
    "                all_articles.append(article_info)\n",
    "    \n",
    "    # Sort by similarity score (descending)\n",
    "    all_articles.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    print(f\"Retrieved {len(all_articles)} relevant articles in total\")\n",
    "    return all_articles\n",
    "\n",
    "# Define our portfolio\n",
    "portfolio = [\"AAPL\", \"TSLA\", \"MSFT\"]\n",
    "\n",
    "# Retrieve articles\n",
    "articles = get_articles_for_portfolio(\n",
    "    tickers=portfolio,\n",
    "    days_back=30,  # Look back 30 days to ensure we find some articles\n",
    "    article_limit=3  # Limit to 3 articles per ticker for this demo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_articles = [article for article in articles if article['ticker']=='TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ca0de",
   "metadata": {},
   "source": [
    "Let's look at a few example articles that we retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tsla_articles:\n",
    "    # Display information about the first few articles\n",
    "    for i, article in enumerate(tsla_articles[:3]):\n",
    "        print(f\"Article #{i+1} - {article['ticker']} (Similarity: {article['similarity']:.4f})\")\n",
    "        print(f\"Title: {article['metadata'].get('title', 'N/A')}\")\n",
    "        print(f\"Source: {article['metadata'].get('source', 'N/A')}\")\n",
    "        print(f\"Date: {article['metadata'].get('pubDate', 'N/A')}\")\n",
    "        print(\"\\nExcerpt:\")\n",
    "        print(article['document'][:300] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No articles found. Make sure your vector database contains financial news articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4d480",
   "metadata": {},
   "source": [
    "## 5. Generate Summaries\n",
    "\n",
    "Now, let's generate summaries for the retrieved articles using both our baseline and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_summaries(articles):\n",
    "#     \"\"\"\n",
    "#     Generate a single summary for all articles using both baseline and fine-tuned models.\n",
    "    \n",
    "#     Args:\n",
    "#         articles: List of articles to summarize\n",
    "        \n",
    "#     Returns:\n",
    "#         Dictionary with combined summaries for all articles\n",
    "#     \"\"\"\n",
    "#     # Combine all article texts\n",
    "#     combined_text = \"\\n\\n\".join([f\"Article {i+1}: {article['document']}\" \n",
    "#                                 for i, article in enumerate(articles)])\n",
    "    \n",
    "#     summaries = {\n",
    "#         \"ticker\": articles[0][\"ticker\"] if articles else \"Unknown\",\n",
    "#         \"article_count\": len(articles)\n",
    "#     }\n",
    "    \n",
    "#     # Generate baseline summary of all articles\n",
    "#     if baseline_chain:\n",
    "#         try:\n",
    "#             summaries[\"baseline_summary\"] = baseline_chain.invoke(combined_text)\n",
    "#             print(f\"Generated baseline summary for {len(articles)} articles\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating baseline summary: {str(e)}\")\n",
    "#             summaries[\"baseline_summary\"] = \"Error generating summary.\"\n",
    "    \n",
    "#     # Generate fine-tuned summary of all articles\n",
    "#     if finetuned_chain:\n",
    "#         try:\n",
    "#             summaries[\"finetuned_summary\"] = finetuned_chain.invoke(combined_text)\n",
    "#             print(f\"Generated fine-tuned summary for {len(articles)} articles\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating fine-tuned summary: {str(e)}\")\n",
    "#             summaries[\"finetuned_summary\"] = \"Error generating summary.\"\n",
    "    \n",
    "#     return summaries\n",
    "\n",
    "# # Generate summaries for all articles\n",
    "# # Note: This may take some time depending on the number of articles and model access\n",
    "# if baseline_chain or finetuned_chain:\n",
    "#     articles_with_summaries = generate_summaries(tsla_articles)  # Just summarize first 3 for demo\n",
    "# else:\n",
    "#     print(\"Cannot generate summaries: models are not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b82ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(articles):\n",
    "    \"\"\"\n",
    "    Generate summaries for a list of articles using both baseline and fine-tuned models.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of articles to summarize\n",
    "        \n",
    "    Returns:\n",
    "        List of articles with summaries added\n",
    "    \"\"\"\n",
    "    for article in tqdm(articles, desc=\"Generating summaries\"):\n",
    "        text = article[\"document\"]\n",
    "        \n",
    "        # Generate baseline summary\n",
    "        if baseline_chain:\n",
    "            try:\n",
    "                article[\"baseline_summary\"] = baseline_chain.invoke(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating baseline summary: {str(e)}\")\n",
    "                article[\"baseline_summary\"] = \"Error generating summary.\"\n",
    "        \n",
    "        # Generate fine-tuned summary\n",
    "        if finetuned_chain:\n",
    "            try:\n",
    "                article[\"finetuned_summary\"] = finetuned_chain.invoke(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating fine-tuned summary: {str(e)}\")\n",
    "                article[\"finetuned_summary\"] = \"Error generating summary.\"\n",
    "        \n",
    "    return articles\n",
    "\n",
    "# Generate summaries for all articles\n",
    "# Note: This may take some time depending on the number of articles and model access\n",
    "if baseline_chain or finetuned_chain:\n",
    "    articles_with_summaries = generate_summaries(tsla_articles)  # Just summarize first 3 for demo\n",
    "else:\n",
    "    print(\"Cannot generate summaries: models are not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_with_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e2f94",
   "metadata": {},
   "source": [
    "Let's look at the generated summaries for one of the articles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919df4c5",
   "metadata": {},
   "source": [
    "## 6. Evaluate Summaries\n",
    "\n",
    "Now, let's evaluate the quality of our summaries using ROUGE metrics. This will give us a quantitative measure of how well our models are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries(articles):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated summaries using Rouge metrics.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of articles with generated summaries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        \"baseline\": {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []},\n",
    "        \"finetuned\": {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []},\n",
    "        \"comparison\": {\"better\": 0, \"worse\": 0, \"same\": 0}\n",
    "    }\n",
    "    \n",
    "    for article in articles:\n",
    "        # Use the article's summary (if available) as reference\n",
    "        reference = article[\"metadata\"].get(\"summary\", \"\")\n",
    "        \n",
    "        if reference and \"baseline_summary\" in article and \"finetuned_summary\" in article:\n",
    "            # Score baseline summary\n",
    "            baseline_scores = scorer.score(reference, article[\"baseline_summary\"])\n",
    "            for metric, score in baseline_scores.items():\n",
    "                metrics[\"baseline\"][metric].append(score.fmeasure)\n",
    "            \n",
    "            # Score fine-tuned summary\n",
    "            finetuned_scores = scorer.score(reference, article[\"finetuned_summary\"]['text'])\n",
    "            for metric, score in finetuned_scores.items():\n",
    "                metrics[\"finetuned\"][metric].append(score.fmeasure)\n",
    "            \n",
    "            # Compare ROUGE-L scores\n",
    "            if finetuned_scores[\"rougeL\"].fmeasure > baseline_scores[\"rougeL\"].fmeasure:\n",
    "                metrics[\"comparison\"][\"better\"] += 1\n",
    "            elif finetuned_scores[\"rougeL\"].fmeasure < baseline_scores[\"rougeL\"].fmeasure:\n",
    "                metrics[\"comparison\"][\"worse\"] += 1\n",
    "            else:\n",
    "                metrics[\"comparison\"][\"same\"] += 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    for model in [\"baseline\", \"finetuned\"]:\n",
    "        for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "            if metrics[model][metric]:\n",
    "                metrics[model][f\"avg_{metric}\"] = np.mean(metrics[model][metric])\n",
    "            else:\n",
    "                metrics[model][f\"avg_{metric}\"] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate summaries\n",
    "if 'articles_with_summaries' in locals() and articles_with_summaries:\n",
    "    evaluation = evaluate_summaries(articles_with_summaries)\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"ROUGE-1 Average (Baseline): {evaluation['baseline'].get('avg_rouge1', 0):.4f}\")\n",
    "    print(f\"ROUGE-2 Average (Baseline): {evaluation['baseline'].get('avg_rouge2', 0):.4f}\")\n",
    "    print(f\"ROUGE-L Average (Baseline): {evaluation['baseline'].get('avg_rougeL', 0):.4f}\")\n",
    "    print()\n",
    "    print(f\"ROUGE-1 Average (Fine-tuned): {evaluation['finetuned'].get('avg_rouge1', 0):.4f}\")\n",
    "    print(f\"ROUGE-2 Average (Fine-tuned): {evaluation['finetuned'].get('avg_rouge2', 0):.4f}\")\n",
    "    print(f\"ROUGE-L Average (Fine-tuned): {evaluation['finetuned'].get('avg_rougeL', 0):.4f}\")\n",
    "    print()\n",
    "    print(f\"Comparison: {evaluation['comparison']['better']} better, {evaluation['comparison']['worse']} worse, {evaluation['comparison']['same']} same\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ae66d",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Let's create a visualization to compare the performance of our baseline and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'evaluation' in locals():\n",
    "    # Create DataFrame for plotting\n",
    "    metrics = [\"avg_rouge1\", \"avg_rouge2\", \"avg_rougeL\"]\n",
    "    baseline_values = [evaluation[\"baseline\"].get(m, 0) for m in metrics]\n",
    "    finetuned_values = [evaluation[\"finetuned\"].get(m, 0) for m in metrics]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
    "        \"Baseline\": baseline_values,\n",
    "        \"Fine-tuned\": finetuned_values\n",
    "    })\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_melted = pd.melt(df, id_vars=[\"Metric\"], var_name=\"Model\", value_name=\"Score\")\n",
    "    \n",
    "    ax = sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=df_melted)\n",
    "    plt.title(\"ROUGE Metrics Comparison\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d069bf1",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Finally, let's save our results to a JSON file for future reference or to display in the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82883a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'articles_with_summaries' in locals() and 'evaluation' in locals():\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"portfolio\": portfolio,\n",
    "        \"articles_count\": len(articles_with_summaries),\n",
    "        \"articles\": articles_with_summaries,\n",
    "        \"evaluation\": evaluation\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    output_file = \"../data/summaries.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Convert non-serializable objects\n",
    "        # The json.dumps won't directly work with numpy values, so we need to convert them\n",
    "        json_str = json.dumps(results, indent=2, default=lambda x: float(x) if isinstance(x, np.float32) else str(x))\n",
    "        f.write(json_str)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48557a",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've walked through the full process of the Financial News Summarization Agent:\n",
    "\n",
    "1. We connected to the vector database to retrieve relevant articles for our portfolio\n",
    "2. We set up baseline and fine-tuned summarization models\n",
    "3. We generated and compared summaries for each article\n",
    "4. We evaluated the quality of summaries using ROUGE metrics\n",
    "5. We visualized the results to compare model performance\n",
    "\n",
    "This process can be automated to run regularly, keeping you updated on the latest financial news related to your portfolio with high-quality summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153b130",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "To extend this project further, you could:\n",
    "\n",
    "1. **Automate news scraping**: Integrate with your Yahoo Finance scraper to continuously update the vector database\n",
    "2. **Improve evaluation**: Add factual consistency checks and hallucination detection\n",
    "3. **Fine-tune models**: Fine-tune a summarization model on your financial news dataset\n",
    "4. **Create alerts**: Set up notifications for significant news about your portfolio\n",
    "5. **Build a dashboard**: Use the news_dashboard.py script to visualize results in a web interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-agent-Iwjhydeb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
